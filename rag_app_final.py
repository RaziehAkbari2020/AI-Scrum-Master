# -*- coding: utf-8 -*-
"""RAG_App_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SQ8EPM95RFb9NkE-l3W-8LfJ_-mWPeyy
"""

# -*- coding: utf-8 -*-
"""RAG_App.py â€” Retrieval-Augmented Generation App with Persistent Memory"""

# ------------------ Imports ------------------
import os, json, uuid
from pathlib import Path


import streamlit as st

if not st.session_state.get("_pg_cfg_set"):
    st.set_page_config(page_title="As A Agile Project Manager Chat with your data", page_icon="ğŸ’¬")
    st.session_state["_pg_cfg_set"] = True
st.image("Logo_UPC_pillars.jpg", width=120)

st.title("As an Agile Project Manager, chat with your data")


# from langchain.schema import Document
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chat_models import init_chat_model
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool
from langchain_core.messages import SystemMessage
from langgraph.graph import MessagesState, StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.sqlite import SqliteSaver  # ğŸ‘ˆ persistent memory

# ------------------ Page ------------------
# st.set_page_config(page_title="As A Scrum Master Chat with your data", page_icon="ğŸ’¬")
# st.title("As A Scrum Master Chat with your data")

# ------------------ API keys ------------------
# âš ï¸ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ù…Ø­Ù„ÛŒ: Ù‚Ø¨Ù„ Ø§Ø² Ø§Ù†ØªØ´Ø§Ø±ØŒ Ù…Ù‚Ø¯Ø§Ø± Ù‡Ø§Ø±Ø¯Ú©Ø¯ Ø±Ø§ Ø­Ø°Ù Ú©Ù† ÛŒØ§ secrets.toml Ø¨Ø³Ø§Ø².
try:
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
except Exception:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

if not (OPENAI_API_KEY and (OPENAI_API_KEY.startswith("sk-") or OPENAI_API_KEY.startswith("sk-proj-"))):
    st.error("OPENAI_API_KEY Ø¯Ø± Secrets ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ ÛŒØ§ Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª.")
    st.stop()

# ------------------ Upload JSONL ------------------
up = st.file_uploader("Upload your data (each line: {page_content, metadata})", type=["jsonl"])
if not up:
    st.info("Please upload the file documents.jsonl.")
    st.stop()

# Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ÛŒ (Ø§Ø³Ù…/Ø³Ø§ÛŒØ² Ù…ØªÙØ§ÙˆØª) Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯ØŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ù¾Ø§Ú© Ú©Ù†
upload_sig = (up.name, getattr(up, "size", None))
if st.session_state.get("last_upload_sig") != upload_sig:
    st.session_state.pop("vector_store", None)
    st.session_state["last_upload_sig"] = upload_sig

# Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ø³Ù†Ø§Ø¯
documents = []
for line in up:
    obj = json.loads(line.decode("utf-8"))
    documents.append(Document(page_content=obj["page_content"], metadata=obj.get("metadata", {})))


# ------------------ Chunking ------------------
text_splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", ".", " "],
    chunk_size=1000,
    chunk_overlap=100,
)
chunks = text_splitter.split_documents(documents)
st.caption(f"ğŸ”¹ Total chunks created: {len(chunks)}")

# ------------------ Embeddings & Vector Store (FAISS) ------------------
embeddings = OpenAIEmbeddings(model="text-embedding-3-large", api_key=OPENAI_API_KEY)

# Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø±Ø§ ÙÙ‚Ø· ÛŒÚ©â€ŒØ¨Ø§Ø± Ø¨Ø³Ø§Ø² Ùˆ Ø¯Ø± Ø³Ø´Ù† Ù†Ú¯Ù‡ Ø¯Ø§Ø± (Ø¬Ù„ÙˆÛŒ Ø±ÛŒâ€ŒØ±Ø§Ù†â€ŒÙ‡Ø§ÛŒ Streamlit)
if "vector_store" not in st.session_state:
    st.session_state["vector_store"] = FAISS.from_documents(chunks, embeddings)
vector_store = st.session_state["vector_store"]

# ------------------ Retrieve tool ------------------
@tool(response_format="content_and_artifact")
def retrieve(query: str):
    """Retrieve user stories related to a query."""
    retrieved_docs = vector_store.similarity_search(query, k=5)
    serialized = "\n\n".join(
        (
            f"ğŸ”¹ User Story ID: {doc.metadata.get('us_id')}\n"
            f"ğŸ“„ Content:\n{doc.page_content}"
        )
        for doc in retrieved_docs
    )
    return serialized, retrieved_docs

# ------------------ LLM ------------------
llm = init_chat_model("gpt-4o-mini", model_provider="openai", api_key=OPENAI_API_KEY)

# ------------------ Steps ------------------
def query_or_respond(state: MessagesState):
    llm_with_tools = llm.bind_tools([retrieve])
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}

tools = ToolNode([retrieve])

def generate(state: MessagesState):
    # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ToolMessages Ø§Ø®ÛŒØ±
    recent_tool_messages = []
    for message in reversed(state["messages"]):
        if message.type == "tool":
            recent_tool_messages.append(message)
        else:
            break
    tool_messages = recent_tool_messages[::-1]

    docs_content = "\n\n".join(doc.content for doc in tool_messages)
    system_message_content = (
        "You are a professional Scrum Master specializing in Agile Project Management in the field of Software Engineering.\n\n"
        "When a user asks a question:\n"
        "- First, explain your reasoning step-by-step before answering.\n"
        "- Specify which fields (e.g., user story, description, priority, acceptance criteria, tasks, effort) you will use to answer the question, and why you chose them.\n"
        "- Justify your reasoning as an Agile expert.\n"
        "- Provide your final answer immediately after your reasoning.\n\n"
        "Here is the retrieved context:\n"
        f"{docs_content}"
    )
    conversation_messages = [
        message for message in state["messages"]
        if message.type in ("human", "system") or (message.type == "ai" and not message.tool_calls)
    ]
    prompt = [SystemMessage(system_message_content)] + conversation_messages
    response = llm.invoke(prompt)
    return {"messages": [response]}

# ------------------ Graph + Persistent Checkpointer ------------------
# graph_builder = StateGraph(MessagesState)
# graph_builder.add_node("router", query_or_respond)
# graph_builder.add_node("tools", tools)
# graph_builder.add_node("generate", generate)
# graph_builder.set_entry_point("router")
# graph_builder.add_edge("router", "tools")
# graph_builder.add_edge("tools", "generate")
# graph_builder.add_edge("generate", END)

# # âœ… Ø­Ø§ÙØ¸Ù‡Ù” Ù¾Ø§ÛŒØ¯Ø§Ø± Ø±ÙˆÛŒ Ø¯ÛŒØ³Ú©
# saver = SqliteSaver.from_conn_string("rag_state.sqlite")
# graph = graph_builder.compile(checkpointer=saver)



# ------------------ Graph + Persistent Checkpointer ------------------
graph_builder = StateGraph(MessagesState)
graph_builder.add_node("router", query_or_respond)
graph_builder.add_node("tools", tools)
graph_builder.add_node("generate", generate)
graph_builder.set_entry_point("router")
graph_builder.add_edge("router", "tools")
graph_builder.add_edge("tools", "generate")
graph_builder.add_edge("generate", END)


# ------------------ Streamlit Chat UI ------------------
# if "messages" not in st.session_state:
#     st.session_state.messages = []  # ØªØ§Ø±ÛŒØ®Ú†Ù‡ UI

# # thread_id Ù¾Ø§ÛŒØ¯Ø§Ø± (Ø¨Ø±Ø§ÛŒ LangGraph)
# if "thread_id" not in st.session_state:
#     st.session_state.thread_id = str(uuid.uuid4())
# config = {"configurable": {"thread_id": st.session_state.thread_id}}

# # Ù†Ù…Ø§ÛŒØ´ ØªØ§Ø±ÛŒØ®Ú†Ù‡
# for m in st.session_state.messages:
#     with st.chat_message(m["role"]):
#         st.markdown(m["content"])

# user_input = st.chat_input("Ask your Question")
# if user_input:
#     st.session_state.messages.append({"role": "user", "content": user_input})
#     with st.chat_message("user"):
#         st.markdown(user_input)

#     # ğŸ‘ˆ Ú©Ù„ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø±Ø§ Ø¨Ù‡ Ú¯Ø±Ø§Ù Ø¨Ø¯Ù‡ØŒ Ù†Ù‡ ÙÙ‚Ø· Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù…
#     history = [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]

#     ai_text = None
#     placeholder = st.chat_message("assistant").empty()
#     for step in graph.stream(
#         {"messages": history},
#         stream_mode="values",
#         config=config,
#     ):
#         msgs = step.get("messages", [])
#         if msgs:
#             last = msgs[-1]
#             if getattr(last, "type", None) == "ai" and getattr(last, "content", ""):
#                 ai_text = last.content
#                 placeholder.markdown(ai_text)

#     if ai_text:
#         st.session_state.messages.append({"role": "assistant", "content": ai_text})

# st.divider()
# if st.button("ğŸ—‘ï¸ Clear chat"):
#     st.session_state.messages = []
#     # thread_id Ø±Ø§ Ù‡Ù… Ø±ÛŒØ³Øª Ù†Ú©Ù† ØªØ§ state Ú¯Ø±Ø§Ù Ø­ÙØ¸ Ø´ÙˆØ¯Ø› Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØŒ Ø§ÛŒÙ† Ø®Ø· Ø±Ø§ Ù‡Ù… Ø¢Ù†Ú©Ø§Ù…Ù†Øª Ú©Ù†:
#     # st.session_state.thread_id = str(uuid.uuid4())
#     st.experimental_rerun()





with SqliteSaver.from_conn_string("rag_state.sqlite") as saver:
    graph = graph_builder.compile(checkpointer=saver)

    # ------------------ Streamlit Chat UI ------------------
    if "messages" not in st.session_state:
        st.session_state.messages = []

    if "thread_id" not in st.session_state:
        st.session_state.thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": st.session_state.thread_id}}

    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    user_input = st.chat_input("Ask your Question")
    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        history = [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]

        ai_text = None
        placeholder = st.chat_message("assistant").empty()
        for step in graph.stream(
            {"messages": history},
            stream_mode="values",
            config=config,
        ):
            msgs = step.get("messages", [])
            if msgs:
                last = msgs[-1]
                if getattr(last, "type", None) == "ai" and getattr(last, "content", ""):
                    ai_text = last.content
                    placeholder.markdown(ai_text)

        if ai_text:
            st.session_state.messages.append({"role": "assistant", "content": ai_text})

    st.divider()
    if st.button("ğŸ—‘ï¸ Clear chat"):
        st.session_state.messages = []
        st.experimental_rerun()
